name: D Central Test Suite Weeks 1-3 (Enhanced)

on:
  push:
    branches: [ main, develop, feature/*, release/* ]
  pull_request:
    branches: [ main, develop ]
  workflow_dispatch:

env:
  CI_LOG_DIR: logs/github-actions
  CI_LOG_LEVEL: info
  CI_LOG_RETENTION_DAYS: 14

jobs:
  #--------------------------------------------------
  # Week 1 Tests
  #--------------------------------------------------
  W1-REPO-01:
    name: W1-REPO-01 Repository Structure Check
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      - name: Set up logging
        run: |
          mkdir -p ${{ env.CI_LOG_DIR }}
          chmod +x scripts/ci/test_logger.sh
      - name: Run directory structure check
        run: |
          source scripts/ci/test_logger.sh
          test_start "W1-REPO-01" "Repository Structure Check"
          log "info" "Validating directory structure against manifest"
          if python3 scripts/ci/tree_assert.py; then
            test_end "W1-REPO-01" "pass"
          else
            test_end "W1-REPO-01" "fail" "Directory structure does not match manifest"
            exit 1
          fi
      - name: Upload logs
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: w1-repo-01-logs
          path: ${{ env.CI_LOG_DIR }}/*.log

  W1-LEGAL-01:
    name: W1-LEGAL-01 Required Legal Docs Check
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Set up logging
        run: |
          mkdir -p ${{ env.CI_LOG_DIR }}
          chmod +x scripts/ci/test_logger.sh
      - name: Check legal docs existence
        run: |
          source scripts/ci/test_logger.sh
          test_start "W1-LEGAL-01" "Required Legal Docs Check"
          
          required_files=(
            "legal/mutual-nda_v1.0.md"
            "legal/privacy-notice_v1.0.md"
            "legal/revenue-share-warrant_v1.md"
            "LICENSE.md"
            "CODE_OF_CONDUCT.md"
          )
          
          missing=0
          missing_files=""
          for file in "${required_files[@]}"; do
            if [ ! -f "$file" ]; then
              log "error" "Missing required file: $file"
              missing=1
              missing_files="$missing_files $file"
            else
              log "success" "Found required file: $file"
              # Also check the file size to ensure it's not empty
              file_size=$(wc -c < "$file" | tr -d ' ')
              log "debug" "File size for $file: $file_size bytes"
              if [ "$file_size" -lt 100 ]; then
                log "warn" "File $file is suspiciously small: $file_size bytes"
              fi
            fi
          done
          
          if [ $missing -eq 1 ]; then
            test_end "W1-LEGAL-01" "fail" "Missing required legal files:$missing_files"
            exit 1
          else
            test_end "W1-LEGAL-01" "pass"
          fi
      - name: Upload logs
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: w1-legal-01-logs
          path: ${{ env.CI_LOG_DIR }}/*.log

  W1-DOCS-01:
    name: W1-DOCS-01 Markdown Linting
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Set up logging
        run: |
          mkdir -p ${{ env.CI_LOG_DIR }}
          chmod +x scripts/ci/test_logger.sh
      - name: markdownlint-cli
        run: |
          source scripts/ci/test_logger.sh
          test_start "W1-DOCS-01" "Markdown Linting"
          
          npm install -g markdownlint-cli2
          
          log "info" "Running markdown linting on all markdown files"
          lint_output=$(markdownlint-cli2 "**/*.md" 2>&1)
          lint_exit_code=$?
          log_data "Markdown lint output" "$lint_output"
          
          if [ $lint_exit_code -eq 0 ]; then
            test_end "W1-DOCS-01" "pass"
          else
            errors_count=$(echo "$lint_output" | grep -c "error")
            test_end "W1-DOCS-01" "fail" "$errors_count markdown lint errors detected"
            exit 1
          fi
      - name: Upload logs
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: w1-docs-01-logs
          path: ${{ env.CI_LOG_DIR }}/*.log

  W1-BRAND-01:
    name: W1-BRAND-01 Brand Assets Check
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Set up logging
        run: |
          mkdir -p ${{ env.CI_LOG_DIR }}
          chmod +x scripts/ci/test_logger.sh
      - name: Check brand assets
        run: |
          source scripts/ci/test_logger.sh
          test_start "W1-BRAND-01" "Brand Assets Check"
          
          log "info" "Checking for SVG logo files"
          svg_files=$(find design/logo/static -name "*.svg" 2>/dev/null)
          svg_files_count=$(echo "$svg_files" | grep -c "\.svg$")
          
          log_data "Found SVG files" "$svg_files"
          
          if [ "$svg_files_count" -lt 2 ]; then
            test_end "W1-BRAND-01" "fail" "Not enough SVG logo files found (minimum 2 required, found $svg_files_count)"
            exit 1
          else
            test_end "W1-BRAND-01" "pass" "Found $svg_files_count SVG logo files"
          fi
      - name: Upload logs
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: w1-brand-01-logs
          path: ${{ env.CI_LOG_DIR }}/*.log

  W1-BRAND-02:
    name: W1-BRAND-02 SVG Optimization Check
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
      - name: Set up logging
        run: |
          mkdir -p ${{ env.CI_LOG_DIR }}
          chmod +x scripts/ci/test_logger.sh
      - name: Install svgo
        run: npm install -g svgo
      - name: Check SVG optimization
        run: |
          source scripts/ci/test_logger.sh
          test_start "W1-BRAND-02" "SVG Optimization Check"
          
          log "info" "Checking SVG optimization potential"
          declare -a optimization_issues=()
          
          for svg in $(find design/logo/static -name "*.svg" 2>/dev/null); do
            log "debug" "Analyzing $svg"
            before_size=$(wc -c < "$svg")
            svgo --disable=removeViewBox "$svg" -o "$svg.optimized" > /dev/null 2>&1
            after_size=$(wc -c < "$svg.optimized")
            rm "$svg.optimized"
            
            savings=$(( 100 - (after_size * 100 / before_size) ))
            log "info" "$svg: Original: $before_size bytes, Potential: $after_size bytes, Savings: $savings%"
            
            if [ "$savings" -gt 10 ]; then
              log "warn" "SVG file $svg could be optimized further (potential $savings% savings)"
              optimization_issues+=("$svg: $savings% potential savings")
            else
              log "success" "SVG file $svg is well optimized"
            fi
          done
          
          if [ ${#optimization_issues[@]} -gt 0 ]; then
            optimization_issues_str=$(printf "%s; " "${optimization_issues[@]}")
            test_end "W1-BRAND-02" "warn" "Some SVGs could be further optimized: ${optimization_issues_str}"
          else
            test_end "W1-BRAND-02" "pass"
          fi
      - name: Upload logs
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: w1-brand-02-logs
          path: ${{ env.CI_LOG_DIR }}/*.log

  W1-TOKEN-01:
    name: W1-TOKEN-01 Design Token File Check
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Set up logging
        run: |
          mkdir -p ${{ env.CI_LOG_DIR }}
          chmod +x scripts/ci/test_logger.sh
      - name: Check design tokens
        run: |
          source scripts/ci/test_logger.sh
          test_start "W1-TOKEN-01" "Design Token File Check"
          
          token_file="design/palette-tokens/design-tokens.json"
          
          if [ ! -f "$token_file" ]; then
            test_end "W1-TOKEN-01" "fail" "Missing design tokens file: $token_file"
            exit 1
          fi
          
          log "success" "Found design tokens file"
          
          # Check if the JSON is valid
          jq_output=$(jq empty "$token_file" 2>&1)
          jq_exit_code=$?
          
          if [ $jq_exit_code -ne 0 ]; then
            log "error" "Invalid JSON in design tokens file"
            log_data "JSON validation error" "$jq_output"
            test_end "W1-TOKEN-01" "fail" "Invalid JSON in design tokens file"
            exit 1
          fi
          
          log "success" "Design tokens file contains valid JSON"
          
          # Check required token categories
          required_categories=("colors" "spacing" "typography")
          missing=0
          missing_categories=""
          
          for category in "${required_categories[@]}"; do
            if ! jq -e ".$category" "$token_file" >/dev/null 2>&1; then
              log "error" "Missing required token category: $category"
              missing=1
              missing_categories="$missing_categories $category"
            else
              log "success" "Found required token category: $category"
              # Check depth of the category
              keys_count=$(jq ".$category | keys | length" "$token_file")
              log "debug" "Category $category has $keys_count keys"
            fi
          done
          
          if [ $missing -eq 1 ]; then
            test_end "W1-TOKEN-01" "fail" "Missing required token categories:$missing_categories"
            exit 1
          else
            # Additional in-depth validation
            colors_count=$(jq '.colors | keys | length' "$token_file")
            log "info" "Found $colors_count color categories in design tokens"
            
            if [ "$colors_count" -lt 3 ]; then
              test_end "W1-TOKEN-01" "warn" "Design tokens have only $colors_count color categories (recommended minimum is 3)"
            else
              test_end "W1-TOKEN-01" "pass"
            fi
          fi
      - name: Upload logs
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: w1-token-01-logs
          path: ${{ env.CI_LOG_DIR }}/*.log

  #--------------------------------------------------
  # Week 2 Tests
  #--------------------------------------------------
  W2-WCAG-01:
    name: W2-WCAG-01 Color Contrast Check
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
      - name: Set up logging
        run: |
          mkdir -p ${{ env.CI_LOG_DIR }}
          chmod +x scripts/ci/test_logger.sh
      - name: Install dependencies
        run: npm install wcag-contrast
      - name: Run contrast check
        run: |
          source scripts/ci/test_logger.sh
          test_start "W2-WCAG-01" "Color Contrast Check"
          
          log "info" "Running WCAG contrast check on design tokens"
          
          contrast_output=$(node scripts/ci/contrast.js 2>&1)
          contrast_exit_code=$?
          log_data "Contrast check output" "$contrast_output"
          
          if [ $contrast_exit_code -eq 0 ]; then
            passed_colors=$(echo "$contrast_output" | grep -o "Checked [0-9]* color tokens" | awk '{print $2}')
            test_end "W2-WCAG-01" "pass" "All $passed_colors colors pass WCAG 2.1 AA contrast requirements"
          else
            failing_count=$(echo "$contrast_output" | grep -c "contrast issues")
            test_end "W2-WCAG-01" "fail" "Found $failing_count colors with contrast issues"
            exit 1
          fi
      - name: Upload logs
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: w2-wcag-01-logs
          path: ${{ env.CI_LOG_DIR }}/*.log

  W2-DOCKER-01:
    name: W2-DOCKER-01 Docker Image Build Check
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Set up logging
        run: |
          mkdir -p ${{ env.CI_LOG_DIR }}
          chmod +x scripts/ci/test_logger.sh
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      - name: Build Edge Gateway Docker image
        run: |
          source scripts/ci/test_logger.sh
          test_start "W2-DOCKER-01" "Docker Image Build Check"
          
          log "info" "Building Docker image for Edge Gateway"
          docker_build_start=$(date +%s)
          docker_output=$(cd code/edge-gateway && docker build -t dcentral/edge-gateway:test . 2>&1)
          docker_exit_code=$?
          docker_build_end=$(date +%s)
          docker_build_time=$((docker_build_end - docker_build_start))
          
          log_data "Docker build output" "$docker_output"
          log "info" "Docker build took $docker_build_time seconds"
          
          if [ $docker_exit_code -eq 0 ]; then
            # Get image size
            image_size=$(docker images dcentral/edge-gateway:test --format "{{.Size}}")
            log "info" "Docker image size: $image_size"
            
            test_end "W2-DOCKER-01" "pass" "Docker image built successfully in $docker_build_time seconds, size: $image_size"
          else
            test_end "W2-DOCKER-01" "fail" "Docker build failed"
            exit 1
          fi
      - name: Upload logs
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: w2-docker-01-logs
          path: ${{ env.CI_LOG_DIR }}/*.log

  W2-SBOM-01:
    name: W2-SBOM-01 SBOM Generation
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Set up logging
        run: |
          mkdir -p ${{ env.CI_LOG_DIR }}
          chmod +x scripts/ci/test_logger.sh
      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'fs'
          format: 'cyclonedx'
          output: 'sbom.cdx.json'
      - name: Upload SBOM
        uses: actions/upload-artifact@v3
        with:
          name: sbom
          path: sbom.cdx.json
      - name: Log SBOM generation
        run: |
          source scripts/ci/test_logger.sh
          test_start "W2-SBOM-01" "SBOM Generation"
          
          if [ -f "sbom.cdx.json" ]; then
            # Extract some data from SBOM for logging
            components_count=$(jq '.components | length' sbom.cdx.json 2>/dev/null || echo "unknown")
            log "info" "SBOM contains $components_count components"
            test_end "W2-SBOM-01" "pass" "SBOM generated successfully with $components_count components"
          else
            test_end "W2-SBOM-01" "fail" "Failed to generate SBOM file"
            exit 1
          fi
      - name: Upload logs
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: w2-sbom-01-logs
          path: ${{ env.CI_LOG_DIR }}/*.log

  W2-SBOM-02:
    name: W2-SBOM-02 License Compatibility Check
    runs-on: ubuntu-latest
    needs: W2-SBOM-01
    steps:
      - uses: actions/checkout@v4
      - name: Set up logging
        run: |
          mkdir -p ${{ env.CI_LOG_DIR }}
          chmod +x scripts/ci/test_logger.sh
      - name: Download SBOM
        uses: actions/download-artifact@v3
        with:
          name: sbom
      - name: Check SBOM for GPL compatibility
        run: |
          source scripts/ci/test_logger.sh
          test_start "W2-SBOM-02" "License Compatibility Check"
          
          log "info" "Checking SBOM for GPL compatibility"
          sbom_check_output=$(bash scripts/ci/sbom_diff_checker_enhanced.sh sbom.cdx.json scripts/ci/gpl_compatible_licenses.txt 2>&1)
          sbom_check_exit_code=$?
          log_data "SBOM license check output" "$sbom_check_output"
          
          if [ $sbom_check_exit_code -eq 0 ]; then
            test_end "W2-SBOM-02" "pass" "No GPL-incompatible licenses found"
          else
            incompatible_count=$(echo "$sbom_check_output" | grep -o "Found [0-9]* GPL-incompatible licenses" | awk '{print $2}')
            test_end "W2-SBOM-02" "fail" "Found $incompatible_count GPL-incompatible licenses"
            exit 1
          fi
      - name: Upload logs
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: w2-sbom-02-logs
          path: ${{ env.CI_LOG_DIR }}/*.log

  W2-GO-01:
    name: W2-GO-01 Go Code Quality Check
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Set up logging
        run: |
          mkdir -p ${{ env.CI_LOG_DIR }}
          chmod +x scripts/ci/test_logger.sh
      - name: Setup Go
        uses: actions/setup-go@v4
        with:
          go-version: '1.21'
      - name: Go code quality checks
        run: |
          source scripts/ci/test_logger.sh
          test_start "W2-GO-01" "Go Code Quality Check"
          
          log "info" "Running Go code quality checks"
          
          # Check if the directory exists
          if [ ! -d "code/edge-gateway" ]; then
            test_end "W2-GO-01" "fail" "Edge Gateway code directory not found"
            exit 1
          fi
          
          # Run Go module verification
          log "info" "Verifying Go modules"
          go_mod_output=$(cd code/edge-gateway && go mod verify 2>&1)
          go_mod_exit_code=$?
          log_data "Go mod verify output" "$go_mod_output"
          
          if [ $go_mod_exit_code -ne 0 ]; then
            test_end "W2-GO-01" "fail" "Go module verification failed"
            exit 1
          fi
          
          log "success" "Go modules verified successfully"
          
          # Build the code
          log "info" "Building Go code"
          go_build_output=$(cd code/edge-gateway && go build -v ./... 2>&1)
          go_build_exit_code=$?
          log_data "Go build output" "$go_build_output"
          
          if [ $go_build_exit_code -ne 0 ]; then
            test_end "W2-GO-01" "fail" "Go build failed"
            exit 1
          fi
          
          log "success" "Go build successful"
          
          # Run tests with race detection and coverage
          log "info" "Running Go tests with race detection and coverage analysis"
          go_test_output=$(cd code/edge-gateway && go test -race -coverprofile=coverage.txt -covermode=atomic ./... 2>&1)
          go_test_exit_code=$?
          log_data "Go test output" "$go_test_output"
          
          if [ $go_test_exit_code -ne 0 ]; then
            test_end "W2-GO-01" "fail" "Go tests failed"
            exit 1
          fi
          
          log "success" "Go tests passed"
          
          # Check coverage
          log "info" "Checking code coverage"
          coverage_output=$(cd code/edge-gateway && go tool cover -func=coverage.txt 2>&1)
          log_data "Coverage output" "$coverage_output"
          
          # Extract total coverage
          coverage=$(cd code/edge-gateway && go tool cover -func=coverage.txt | grep total | awk '{print $3}' | tr -d '%')
          log "info" "Total code coverage: $coverage%"
          
          if (( $(echo "$coverage < 70.0" | bc -l) )); then
            test_end "W2-GO-01" "fail" "Code coverage below 70%: $coverage%"
            exit 1
          else
            test_end "W2-GO-01" "pass" "All Go code quality checks passed, coverage: $coverage%"
          fi
      - name: Upload logs
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: w2-go-01-logs
          path: ${{ env.CI_LOG_DIR }}/*.log

  W2-HELM-01:
    name: W2-HELM-01 Helm Chart Validation
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Set up logging
        run: |
          mkdir -p ${{ env.CI_LOG_DIR }}
          chmod +x scripts/ci/test_logger.sh
      - name: Set up Helm
        uses: azure/setup-helm@v3
        with:
          version: 'v3.12.0'
      - name: Lint Helm chart
        run: |
          source scripts/ci/test_logger.sh
          test_start "W2-HELM-01" "Helm Chart Validation"
          
          log "info" "Linting Helm chart"
          
          # Check if the chart directory exists
          if [ ! -d "code/helm/edge-gateway-chart" ]; then
            test_end "W2-HELM-01" "fail" "Helm chart directory not found"
            exit 1
          fi
          
          # Lint the chart
          helm_lint_output=$(helm lint code/helm/edge-gateway-chart 2>&1)
          helm_lint_exit_code=$?
          log_data "Helm lint output" "$helm_lint_output"
          
          if [ $helm_lint_exit_code -ne 0 ]; then
            test_end "W2-HELM-01" "fail" "Helm lint found errors"
            exit 1
          fi
          
          log "success" "Helm chart passed linting"
          test_end "W2-HELM-01" "pass" "Helm chart validated successfully"
      - name: Template Helm chart and validate
        run: |
          source scripts/ci/test_logger.sh
          test_start "W2-HELM-01-K8S" "Kubernetes Resources Validation"
          
          log "info" "Validating templated Kubernetes resources"
          helm_template_output=$(helm template code/helm/edge-gateway-chart > templated.yaml 2>&1)
          log_data "Helm template output" "$helm_template_output"
          
          if ! kubectl create --dry-run=client -f templated.yaml > /dev/null; then
            log "error" "Kubernetes validation of Helm templates failed"
            kubectl_validate_output=$(kubectl create --dry-run=client -f templated.yaml 2>&1)
            log_data "Kubectl validation output" "$kubectl_validate_output"
            test_end "W2-HELM-01-K8S" "fail" "Helm chart templates contain invalid Kubernetes resources"
            exit 1
          else
            log "success" "Helm chart templates validated successfully"
            test_end "W2-HELM-01-K8S" "pass" "Kubernetes resources validated successfully"
          fi
      - name: Upload logs
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: w2-helm-01-logs
          path: ${{ env.CI_LOG_DIR }}/*.log

  #--------------------------------------------------
  # Week 3 Tests
  #--------------------------------------------------
  W3-K8S-01:
    name: W3-K8S-01 Kubernetes Manifest Validation
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Set up logging
        run: |
          mkdir -p ${{ env.CI_LOG_DIR }}
          chmod +x scripts/ci/test_logger.sh
      - name: Validate Kubernetes manifests
        uses: stefanprodan/kube-tools@v1
        with:
          kubectl: 1.27.3
          command: |
            source $(pwd)/scripts/ci/test_logger.sh
            test_start "W3-K8S-01" "Kubernetes Manifest Validation"
            
            log "info" "Validating Kubernetes manifests"
            
            # Check if the templates directory exists
            if [ ! -d "code/helm/edge-gateway-chart/templates" ]; then
              test_end "W3-K8S-01" "fail" "Kubernetes templates directory not found"
              exit 1
            fi
            
            # Count the number of YAML files
            yaml_files=$(find code/helm/edge-gateway-chart/templates -name "*.yaml" 2>/dev/null)
            yaml_files_count=$(echo "$yaml_files" | grep -c "\.yaml$")
            log "info" "Found $yaml_files_count YAML manifest files"
            
            if [ "$yaml_files_count" -eq 0 ]; then
              test_end "W3-K8S-01" "fail" "No YAML manifest files found"
              exit 1
            fi
            
            # Validate each manifest
            valid_count=0
            invalid_count=0
            invalid_files=""
            
            for manifest in $(find code/helm/edge-gateway-chart/templates -name "*.yaml"); do
              log "debug" "Validating $manifest"
              kubectl_output=$(kubectl create --dry-run=client -f $manifest 2>&1)
              kubectl_exit_code=$?
              
              if [ $kubectl_exit_code -eq 0 ]; then
                log "success" "$manifest is valid"
                valid_count=$((valid_count + 1))
              else
                log "warn" "$manifest not directly valid, but may be valid after rendering"
                log_data "Kubectl output for $manifest" "$kubectl_output"
                invalid_count=$((invalid_count + 1))
                invalid_files="$invalid_files $(basename $manifest)"
              fi
            done
            
            if [ $invalid_count -eq 0 ]; then
              test_end "W3-K8S-01" "pass" "All $valid_count Kubernetes manifests validated successfully"
            else
              # Not failing the test as templates often contain variables that are replaced during rendering
              test_end "W3-K8S-01" "warn" "$invalid_count/$yaml_files_count manifests require rendering:$invalid_files"
            fi
      - name: Upload logs
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: w3-k8s-01-logs
          path: ${{ env.CI_LOG_DIR }}/*.log

  W3-SEC-01:
    name: W3-SEC-01 Container Security Scan
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Set up logging
        run: |
          mkdir -p ${{ env.CI_LOG_DIR }}
          chmod +x scripts/ci/test_logger.sh
      - name: Build image
        run: |
          source scripts/ci/test_logger.sh
          test_start "W3-SEC-01-BUILD" "Container Image Build"
          
          log "info" "Building Docker image for security scan"
          docker_output=$(docker build -t dcentral/edge-gateway:test code/edge-gateway 2>&1)
          docker_exit_code=$?
          log_data "Docker build output" "$docker_output"
          
          if [ $docker_exit_code -eq 0 ]; then
            test_end "W3-SEC-01-BUILD" "pass" "Docker image built successfully"
          else
            test_end "W3-SEC-01-BUILD" "fail" "Docker build failed"
            exit 1
          fi
      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: 'dcentral/edge-gateway:test'
          format: 'table'
          exit-code: '0' # Don't fail the build yet
          ignore-unfixed: true
          vuln-type: 'os,library'
          severity: 'CRITICAL,HIGH'
          output: 'trivy-results.txt'
      - name: Process Trivy results
        run: |
          source scripts/ci/test_logger.sh
          test_start "W3-SEC-01" "Container Security Scan"
          
          log "info" "Processing security scan results"
          
          # Check if results file exists
          if [ ! -f "trivy-results.txt" ]; then
            test_end "W3-SEC-01" "fail" "Security scan results not found"
            exit 1
          fi
          
          trivy_output=$(cat trivy-results.txt)
          log_data "Trivy security scan output" "$trivy_output"
          
          # Count vulnerabilities
          if grep -q "Total: 0" trivy-results.txt; then
            test_end "W3-SEC-01" "pass" "No HIGH or CRITICAL vulnerabilities found"
          else
            # Extract vulnerability counts
            high_count=$(grep "HIGH: " trivy-results.txt | awk '{print $2}')
            critical_count=$(grep "CRITICAL: " trivy-results.txt | awk '{print $2}')
            high_count=${high_count:-0}
            critical_count=${critical_count:-0}
            
            if [ $critical_count -gt 0 ]; then
              test_end "W3-SEC-01" "fail" "Found $critical_count CRITICAL and $high_count HIGH vulnerabilities"
              exit 1
            elif [ $high_count -gt 5 ]; then
              test_end "W3-SEC-01" "fail" "Found $high_count HIGH vulnerabilities (threshold is 5)"
              exit 1
            else
              test_end "W3-SEC-01" "warn" "Found $high_count HIGH vulnerabilities (within acceptable limits)"
            fi
          fi
      - name: Upload logs
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: w3-sec-01-logs
          path: ${{ env.CI_LOG_DIR }}/*.log
      - name: Upload scan results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: security-scan-results
          path: trivy-results.txt

  W3-PERF-01:
    name: W3-PERF-01 MQTT Performance Test
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Set up logging
        run: |
          mkdir -p ${{ env.CI_LOG_DIR }}
          chmod +x scripts/ci/test_logger.sh
      - name: Run k6 MQTT test
        uses: grafana/k6-action@v0.3.1
        with:
          filename: code/tests/perf/mqtt_loadtest.js
          flags: --out json=mqtt-results.json --vus 10 --duration 30s
      - name: Process k6 results
        run: |
          source scripts/ci/test_logger.sh
          test_start "W3-PERF-01" "MQTT Performance Test"
          
          log "info" "Processing MQTT performance test results"
          
          # Check if results file exists
          if [ ! -f "mqtt-results.json" ]; then
            test_end "W3-PERF-01" "fail" "MQTT performance test results not found"
            exit 1
          fi
          
          # Extract metrics if available
          mqtt_loss=$(jq '.metrics.mqtt_loss_percentage.values.value' mqtt-results.json 2>/dev/null || echo "unknown")
          log "info" "MQTT message loss: $mqtt_loss%"
          
          p95_published=$(jq '.metrics.mqtt_published.values.p\(95\)' mqtt-results.json 2>/dev/null || echo "unknown")
          log "info" "MQTT publish p95 latency: $p95_published ms"
          
          p95_received=$(jq '.metrics.mqtt_received.values.p\(95\)' mqtt-results.json 2>/dev/null || echo "unknown")
          log "info" "MQTT receive p95 latency: $p95_received ms"
          
          if [ "$mqtt_loss" != "unknown" ] && (( $(echo "$mqtt_loss > 0.5" | bc -l) )); then
            test_end "W3-PERF-01" "fail" "MQTT message loss exceeds threshold: $mqtt_loss%"
            exit 1
          else
            test_end "W3-PERF-01" "pass" "MQTT performance test passed, p95 latencies: publish=$p95_published ms, receive=$p95_received ms"
          fi
      - name: Upload logs
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: w3-perf-01-logs
          path: ${{ env.CI_LOG_DIR }}/*.log
      - name: Upload performance results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: mqtt-performance-results
          path: mqtt-results.json

  W3-PERF-02:
    name: W3-PERF-02 REST API Performance Test
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Set up logging
        run: |
          mkdir -p ${{ env.CI_LOG_DIR }}
          chmod +x scripts/ci/test_logger.sh
      - name: Run k6 REST API test
        uses: grafana/k6-action@v0.3.1
        with:
          filename: code/tests/perf/rest_loadtest.js
          flags: --out json=rest-results.json --vus 10 --duration 30s
      - name: Process k6 results
        run: |
          source scripts/ci/test_logger.sh
          test_start "W3-PERF-02" "REST API Performance Test"
          
          log "info" "Processing REST API performance test results"
          
          # Check if results file exists
          if [ ! -f "rest-results.json" ]; then
            test_end "W3-PERF-02" "fail" "REST API performance test results not found"
            exit 1
          fi
          
          # Extract metrics
          p95_latency=$(jq '.metrics.http_req_duration.values.p\(95\)' rest-results.json 2>/dev/null || echo "unknown")
          log "info" "REST API p95 latency: $p95_latency ms"
          
          error_rate=$(jq '.metrics.error_rate.values.rate' rest-results.json 2>/dev/null || echo "unknown")
          log "info" "REST API error rate: $error_rate"
          
          if [ "$p95_latency" != "unknown" ] && (( $(echo "$p95_latency > 200" | bc -l) )); then
            test_end "W3-PERF-02" "fail" "REST API latency exceeds threshold: $p95_latency ms"
            exit 1
          elif [ "$error_rate" != "unknown" ] && (( $(echo "$error_rate > 0.01" | bc -l) )); then
            test_end "W3-PERF-02" "fail" "REST API error rate exceeds threshold: $error_rate"
            exit 1
          else
            test_end "W3-PERF-02" "pass" "REST API performance test passed, p95 latency: $p95_latency ms, error rate: $error_rate"
          fi
      - name: Upload logs
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: w3-perf-02-logs
          path: ${{ env.CI_LOG_DIR }}/*.log
      - name: Upload performance results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: rest-performance-results
          path: rest-results.json

  W3-GH-01:
    name: W3-GH-01 GitHub Workflows Check
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Set up logging
        run: |
          mkdir -p ${{ env.CI_LOG_DIR }}
          chmod +x scripts/ci/test_logger.sh
      - name: Check GitHub Workflows
        run: |
          source scripts/ci/test_logger.sh
          test_start "W3-GH-01" "GitHub Workflows Check"
          
          log "info" "Checking GitHub workflow files"
          
          mkdir -p .github/workflows
          
          workflow_files=$(find .github/workflows -name "*.yml" -o -name "*.yaml" 2>/dev/null)
          workflow_files_count=$(echo "$workflow_files" | grep -c "\.\(yml\|yaml\)$")
          log_data "Found workflow files" "$workflow_files"
          
          if [ "$workflow_files_count" -eq 0 ]; then
            test_end "W3-GH-01" "fail" "No GitHub workflow files found"
            exit 1
          fi
          
          log "success" "Found $workflow_files_count GitHub workflow files"
          
          # Validate workflow files
          invalid_count=0
          invalid_files=""
          
          for file in $(find .github/workflows -name "*.yml" -o -name "*.yaml" 2>/dev/null); do
            log "debug" "Validating $file"
            
            # Check if it contains essential workflow elements
            if grep -q "name:" "$file" && grep -q "runs-on:" "$file" && grep -q "steps:" "$file"; then
              log "success" "Workflow file $file contains required elements"
              
              # Count jobs in the workflow
              jobs_count=$(grep -c "jobs:" "$file")
              log "info" "Workflow file $file contains $jobs_count job definitions"
            else
              log "warn" "Workflow file $file may be missing essential workflow elements"
              invalid_count=$((invalid_count + 1))
              invalid_files="$invalid_files $(basename $file) (missing elements)"
            fi
          done
          
          if [ $invalid_count -eq 0 ]; then
            test_end "W3-GH-01" "pass" "All $workflow_files_count workflow files are valid"
          else
            test_end "W3-GH-01" "fail" "Found $invalid_count invalid workflow files:$invalid_files"
            exit 1
          fi
      - name: Upload logs
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: w3-gh-01-logs
          path: ${{ env.CI_LOG_DIR }}/*.log

  # Final summary job that runs after all others
  summary:
    name: Test Summary Report
    runs-on: ubuntu-latest
    needs: [W1-REPO-01, W1-LEGAL-01, W1-DOCS-01, W1-BRAND-01, W1-BRAND-02, W1-TOKEN-01, 
            W2-WCAG-01, W2-DOCKER-01, W2-SBOM-01, W2-SBOM-02, W2-GO-01, W2-HELM-01,
            W3-K8S-01, W3-SEC-01, W3-PERF-01, W3-PERF-02, W3-GH-01]
    if: always()
    steps:
      - uses: actions/checkout@v4
      - name: Download all workflow run artifacts
        uses: actions/download-artifact@v3
        with:
          path: artifacts
      - name: Generate summary report
        run: |
          echo "# Test Suite Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Week 1 Tests" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Test ID | Status | Details |" >> $GITHUB_STEP_SUMMARY
          echo "|---------|--------|---------|" >> $GITHUB_STEP_SUMMARY
          
          # Process Week 1 tests
          for test_id in W1-REPO-01 W1-LEGAL-01 W1-DOCS-01 W1-BRAND-01 W1-BRAND-02 W1-TOKEN-01; do
            if [ -d "artifacts/${test_id}-logs" ]; then
              status_files=$(find artifacts/${test_id}-logs -name "*_$test_id.log" | head -1)
              if [ -n "$status_files" ]; then
                if grep -q "PASS: $test_id" $status_files; then
                  echo "| $test_id | ✅ Pass | $(grep "PASS: $test_id" $status_files | sed 's/.*PASS: [^ ]* (//' | sed 's/).*//')" >> $GITHUB_STEP_SUMMARY
                elif grep -q "WARN: $test_id" $status_files; then
                  echo "| $test_id | ⚠️ Warning | $(grep "WARN: $test_id" $status_files | sed 's/.*WARN: [^ ]* (//' | sed 's/).*//')" >> $GITHUB_STEP_SUMMARY
                elif grep -q "FAIL: $test_id" $status_files; then
                  echo "| $test_id | ❌ Fail | $(grep "FAIL: $test_id" $status_files | sed 's/.*FAIL: [^ ]* (//' | sed 's/).*//')" >> $GITHUB_STEP_SUMMARY
                else
                  echo "| $test_id | ❓ Unknown | No status found in logs" >> $GITHUB_STEP_SUMMARY
                fi
              else
                echo "| $test_id | ❓ Unknown | No log file found" >> $GITHUB_STEP_SUMMARY
              fi
            else
              echo "| $test_id | ❓ Unknown | No logs directory found" >> $GITHUB_STEP_SUMMARY
            fi
          done
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Week 2 Tests" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Test ID | Status | Details |" >> $GITHUB_STEP_SUMMARY
          echo "|---------|--------|---------|" >> $GITHUB_STEP_SUMMARY
          
          # Process Week 2 tests
          for test_id in W2-WCAG-01 W2-DOCKER-01 W2-SBOM-01 W2-SBOM-02 W2-GO-01 W2-HELM-01; do
            if [ -d "artifacts/${test_id}-logs" ]; then
              status_files=$(find artifacts/${test_id}-logs -name "*_$test_id.log" | head -1)
              if [ -n "$status_files" ]; then
                if grep -q "PASS: $test_id" $status_files; then
                  echo "| $test_id | ✅ Pass | $(grep "PASS: $test_id" $status_files | sed 's/.*PASS: [^ ]* (//' | sed 's/).*//')" >> $GITHUB_STEP_SUMMARY
                elif grep -q "WARN: $test_id" $status_files; then
                  echo "| $test_id | ⚠️ Warning | $(grep "WARN: $test_id" $status_files | sed 's/.*WARN: [^ ]* (//' | sed 's/).*//')" >> $GITHUB_STEP_SUMMARY
                elif grep -q "FAIL: $test_id" $status_files; then
                  echo "| $test_id | ❌ Fail | $(grep "FAIL: $test_id" $status_files | sed 's/.*FAIL: [^ ]* (//' | sed 's/).*//')" >> $GITHUB_STEP_SUMMARY
                else
                  echo "| $test_id | ❓ Unknown | No status found in logs" >> $GITHUB_STEP_SUMMARY
                fi
              else
                echo "| $test_id | ❓ Unknown | No log file found" >> $GITHUB_STEP_SUMMARY
              fi
            else
              echo "| $test_id | ❓ Unknown | No logs directory found" >> $GITHUB_STEP_SUMMARY
            fi
          done
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Week 3 Tests" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Test ID | Status | Details |" >> $GITHUB_STEP_SUMMARY
          echo "|---------|--------|---------|" >> $GITHUB_STEP_SUMMARY
          
          # Process Week 3 tests
          for test_id in W3-K8S-01 W3-SEC-01 W3-PERF-01 W3-PERF-02 W3-GH-01; do
            if [ -d "artifacts/${test_id}-logs" ]; then
              status_files=$(find artifacts/${test_id}-logs -name "*_$test_id.log" | head -1)
              if [ -n "$status_files" ]; then
                if grep -q "PASS: $test_id" $status_files; then
                  echo "| $test_id | ✅ Pass | $(grep "PASS: $test_id" $status_files | sed 's/.*PASS: [^ ]* (//' | sed 's/).*//')" >> $GITHUB_STEP_SUMMARY
                elif grep -q "WARN: $test_id" $status_files; then
                  echo "| $test_id | ⚠️ Warning | $(grep "WARN: $test_id" $status_files | sed 's/.*WARN: [^ ]* (//' | sed 's/).*//')" >> $GITHUB_STEP_SUMMARY
                elif grep -q "FAIL: $test_id" $status_files; then
                  echo "| $test_id | ❌ Fail | $(grep "FAIL: $test_id" $status_files | sed 's/.*FAIL: [^ ]* (//' | sed 's/).*//')" >> $GITHUB_STEP_SUMMARY
                else
                  echo "| $test_id | ❓ Unknown | No status found in logs" >> $GITHUB_STEP_SUMMARY
                fi
              else
                echo "| $test_id | ❓ Unknown | No log file found" >> $GITHUB_STEP_SUMMARY
              fi
            else
              echo "| $test_id | ❓ Unknown | No logs directory found" >> $GITHUB_STEP_SUMMARY
            fi
          done
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Performance Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### MQTT Performance" >> $GITHUB_STEP_SUMMARY
          if [ -f "artifacts/mqtt-performance-results/mqtt-results.json" ]; then
            mqtt_loss=$(jq '.metrics.mqtt_loss_percentage.values.value // "Not measured"' artifacts/mqtt-performance-results/mqtt-results.json)
            p95_published=$(jq '.metrics.mqtt_published.values."p(95)" // "Not measured"' artifacts/mqtt-performance-results/mqtt-results.json)
            p95_received=$(jq '.metrics.mqtt_received.values."p(95)" // "Not measured"' artifacts/mqtt-performance-results/mqtt-results.json)
            echo "- Message loss: $mqtt_loss%" >> $GITHUB_STEP_SUMMARY
            echo "- Publish p95 latency: $p95_published ms" >> $GITHUB_STEP_SUMMARY
            echo "- Receive p95 latency: $p95_received ms" >> $GITHUB_STEP_SUMMARY
          else
            echo "No MQTT performance results available" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### REST API Performance" >> $GITHUB_STEP_SUMMARY
          if [ -f "artifacts/rest-performance-results/rest-results.json" ]; then
            p95_latency=$(jq '.metrics.http_req_duration.values."p(95)" // "Not measured"' artifacts/rest-performance-results/rest-results.json)
            error_rate=$(jq '.metrics.error_rate.values.rate // "Not measured"' artifacts/rest-performance-results/rest-results.json)
            echo "- p95 latency: $p95_latency ms" >> $GITHUB_STEP_SUMMARY
            echo "- Error rate: $error_rate" >> $GITHUB_STEP_SUMMARY
          else
            echo "No REST API performance results available" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Security Scan Results" >> $GITHUB_STEP_SUMMARY
          if [ -f "artifacts/security-scan-results/trivy-results.txt" ]; then
            echo "```" >> $GITHUB_STEP_SUMMARY
            head -20 artifacts/security-scan-results/trivy-results.txt >> $GITHUB_STEP_SUMMARY
            echo "```" >> $GITHUB_STEP_SUMMARY
            echo "[Full scan results available in workflow artifacts]" >> $GITHUB_STEP_SUMMARY
          else
            echo "No security scan results available" >> $GITHUB_STEP_SUMMARY
          fi
      - name: Upload summary report
        uses: actions/upload-artifact@v3
        with:
          name: test-summary-report
          path: $GITHUB_STEP_SUMMARY